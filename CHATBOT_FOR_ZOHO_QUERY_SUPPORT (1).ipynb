{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1qPNQp-RAJW"
      },
      "outputs": [],
      "source": [
        "!pip install openai requests beautifulsoup4 PyPDF2 gradio matplotlib pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRI0A6OVQ4IG"
      },
      "outputs": [],
      "source": [
        "!pip install readability-lxml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gxgUX8WVRYDF"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-cpu\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkEeufzcRiXt"
      },
      "outputs": [],
      "source": [
        "!pip install playwright\n",
        "!playwright install\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FEylT7wRJrI2"
      },
      "outputs": [],
      "source": [
        "# STEP 1: Install required packages (ensure these are installed once)\n",
        "# If running on Colab/Kaggle or a fresh environment, run these in your notebook/terminal:\n",
        "# !pip install openai transformers requests beautifulsoup4 PyPDF2 gradio matplotlib pandas readability-lxml faiss-cpu sentence-transformers playwright openpyxl\n",
        "# After installing playwright, you MUST also install its browser binaries:\n",
        "# !playwright install\n",
        "\n",
        "# STEP 2: Imports and setup\n",
        "from openai import OpenAI\n",
        "from transformers import pipeline\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from readability import Document\n",
        "from PyPDF2 import PdfReader\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import io, base64\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import gradio as gr\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import textwrap\n",
        "import torch\n",
        "from playwright.sync_api import sync_playwright\n",
        "import threading\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "import os\n",
        "import json\n",
        "\n",
        "# --- Define paths for cached data ---\n",
        "# These will now be product-specific\n",
        "BASE_CACHE_DIR = \"cached_knowledge_bases\"\n",
        "os.makedirs(BASE_CACHE_DIR, exist_ok=True)\n",
        "\n",
        "# --- Define URL lists for each Zoho product ---\n",
        "zoho_books_urls = [\n",
        "    \"https://www.zoho.com/in/books/help/\",\n",
        "    \"https://www.zoho.com/in/books/kb/\",\n",
        "    \"https://www.zoho.com/books/api/v3/\",\n",
        "    \"https://www.zoho.com/in/books/welcome-guide.html\",\n",
        "    \"https://www.zoho.com/in/books/accounting-software-features/\"\n",
        "]\n",
        "\n",
        "zoho_inventory_urls = [\n",
        "    \"https://www.zoho.com/in/inventory/help/\",\n",
        "    \"https://www.zoho.com/in/inventory/kb/\", # Added from user input\n",
        "    \"https://www.zoho.com/inventory/api/v1/\" # Added from user input\n",
        "]\n",
        "\n",
        "zoho_payroll_urls = [\n",
        "    \"https://www.zoho.com/in/payroll/help/\",\n",
        "    \"https://www.zoho.com/in/payroll/kb/\" # Added from user input\n",
        "]\n",
        "\n",
        "zoho_creator_urls = [\n",
        "    \"https://www.zoho.com/creator/help/\"\n",
        "]\n",
        "\n",
        "zoho_analytics_urls = [\n",
        "    \"https://www.zoho.com/analytics/help/overview.html\",\n",
        "    \"https://www.zoho.com/analytics/help/\",\n",
        "    \"https://www.zoho.com/analytics/api/v2/\"\n",
        "]\n",
        "\n",
        "# NEW URL LISTS FOR ADDITIONAL PRODUCTS\n",
        "zoho_people_urls = [\n",
        "    \"https://help.zoho.com/portal/en/kb/people/\"\n",
        "]\n",
        "\n",
        "zoho_recruit_urls = [\n",
        "    \"https://help.zoho.com/portal/en/kb/recruit/\"\n",
        "]\n",
        "\n",
        "zoho_crm_urls = [\n",
        "    \"https://help.zoho.com/portal/en/kb/crm/\"\n",
        "]\n",
        "\n",
        "# --- Integrate OpenAI API Key directly into the code ---\n",
        "OPENAI_API_KEY = \"\"\n",
        "# STEP 3: QA Pipelines\n",
        "device = 0 if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device for local models: {'GPU (ID 0)' if isinstance(device, int) else 'CPU'}\")\n",
        "\n",
        "hf_pipeline = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=\"google/flan-t5-base\",\n",
        "    max_length=512,\n",
        "    truncation=True,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# STEP 4: Text extraction utils\n",
        "def _parse_html_to_readable_text(html_content):\n",
        "    \"\"\"Extracts readable text content from given HTML.\"\"\"\n",
        "    try:\n",
        "        if not html_content or len(html_content.strip()) < 100:\n",
        "            return \"[Error: HTML content too short or empty for parsing]\"\n",
        "        readable_html = Document(html_content).summary()\n",
        "        soup = BeautifulSoup(readable_html, 'html.parser')\n",
        "        text = soup.get_text(separator=\"\\n\", strip=True)\n",
        "        return text if text else \"[Error: No readable text extracted from HTML]\"\n",
        "    except Exception as e:\n",
        "        return f\"[Error parsing HTML]: {e}\"\n",
        "\n",
        "def extract_text_from_file(file_path): # Changed to file_path from file object\n",
        "    \"\"\"Extracts text from a PDF or TXT file path.\"\"\"\n",
        "    if file_path.endswith(\".pdf\"):\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            reader = PdfReader(f)\n",
        "            return \"\\n\".join([p.extract_text() for p in reader.pages if p.extract_text()])\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "# STEP 5: Crawling utility (still necessary to fetch content from URLs)\n",
        "def crawl_single_url(base_url):\n",
        "    \"\"\"\n",
        "    Crawls a single URL using Playwright.\n",
        "    Returns a single (url, text_content) tuple.\n",
        "    \"\"\"\n",
        "    if not isinstance(base_url, str) or not base_url.startswith(\"http\"):\n",
        "        return (base_url, f\"[Error: Invalid URL format: {base_url}]\")\n",
        "\n",
        "    print(f\"Processing: {base_url}\")\n",
        "    try:\n",
        "        with sync_playwright() as p:\n",
        "            browser = p.chromium.launch()\n",
        "            page = browser.new_page()\n",
        "            page.goto(base_url, wait_until='load', timeout=60000)\n",
        "            page.wait_for_timeout(2000) # Wait for any dynamic JS content\n",
        "            html_content = page.content()\n",
        "            extracted_text = _parse_html_to_readable_text(html_content)\n",
        "            browser.close()\n",
        "            return (base_url, extracted_text)\n",
        "    except Exception as e:\n",
        "        error_message = f\"[Error crawling page]: {e}\"\n",
        "        print(f\"   ‚ùå Error at {base_url}: {e}\")\n",
        "        return (base_url, error_message)\n",
        "\n",
        "# STEP 6: Chart rendering\n",
        "def generate_chart_from_csv_text(csv_text):\n",
        "    try:\n",
        "        # This function assumes CSV data is present directly in the context text.\n",
        "        # It's an advanced feature and might need fine-tuning based on actual CSV formats.\n",
        "        df = pd.read_csv(io.StringIO(csv_text))\n",
        "        if df.empty or len(df.columns) < 2:\n",
        "            return \"<b>Chart Error:</b> Not enough data to generate chart.\"\n",
        "        fig, ax = plt.subplots(figsize=(10, 5))\n",
        "        df.set_index(df.columns[0]).plot(kind=\"bar\", ax=ax)\n",
        "        plt.title(\"Chart from Tabular Data\", fontsize=16)\n",
        "        plt.xlabel(df.columns[0], fontsize=12)\n",
        "        plt.ylabel(\"Value\", fontsize=12)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.legend(title=\"Metrics\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        plt.tight_layout()\n",
        "        buf = io.BytesIO()\n",
        "        plt.savefig(buf, format=\"png\")\n",
        "        plt.close(fig)\n",
        "        buf.seek(0)\n",
        "        encoded = base64.b64encode(buf.read()).decode(\"utf-8\")\n",
        "        return f'<img src=\"data:image/png;base64,{encoded}\"/>'\n",
        "    except Exception as e:\n",
        "        return f\"<b>Chart Error:</b> An unexpected error occurred: {e}\"\n",
        "\n",
        "# STEP 7: Context store and FAISS setup\n",
        "# These will be populated based on the loaded product's knowledge base\n",
        "global_context = {\"text\": \"\"} # Store combined raw text for current active KB\n",
        "chunk_texts = [] # Store text chunks for retrieval for current active KB\n",
        "vector_index = None # FAISS index for current active KB\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def chunk_text(text, chunk_size=300, overlap=50):\n",
        "    words = text.split()\n",
        "    return [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size - overlap) if len(words[i:i+chunk_size]) > 20]\n",
        "\n",
        "def build_faiss_index(text_chunks):\n",
        "    global vector_index, chunk_texts\n",
        "    chunk_texts = text_chunks # Update the global chunks for the currently loaded KB\n",
        "    if not chunk_texts:\n",
        "        vector_index = None\n",
        "        return \"No text found to build index.\"\n",
        "    try:\n",
        "        embeddings = embed_model.encode(chunk_texts, show_progress_bar=False)\n",
        "        vector_index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "        vector_index.add(embeddings)\n",
        "        return f\"Index built with {len(chunk_texts)} chunks.\"\n",
        "    except Exception as e:\n",
        "        vector_index = None\n",
        "        return f\"Error building index: {e}\"\n",
        "\n",
        "def retrieve_top_chunks(query, top_k=5):\n",
        "    if not vector_index: return \"No document content loaded or indexed for retrieval.\"\n",
        "    query_vec = embed_model.encode([query])\n",
        "    _, indices = vector_index.search(query_vec, top_k)\n",
        "    return \"\\n---\\n\".join([chunk_texts[i] for i in indices[0]])\n",
        "\n",
        "# STEP 8: QA logic\n",
        "def ask_question(query, model_type):\n",
        "    retrieved_context = retrieve_top_chunks(query)\n",
        "    if \"No document content loaded\" in retrieved_context:\n",
        "        return \"I can't answer right now. Please ensure content is loaded and indexed correctly by clicking a 'Load' button.\"\n",
        "\n",
        "    prompt = f\"Using the following context, answer the question accurately. If the answer is not in the context, state that you cannot answer based on the provided information.\\n\\nContext:\\n{retrieved_context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "    if model_type == \"Hugging Face (Free)\":\n",
        "        result = hf_pipeline(prompt)\n",
        "        return textwrap.fill(result[0]['generated_text'].strip(), width=100)\n",
        "    elif model_type == \"OpenAI (GPT)\":\n",
        "        if not OPENAI_API_KEY or OPENAI_API_KEY == \"YOUR_OPENAI_API_KEY_HERE\":\n",
        "            return \"‚ö†Ô∏è OpenAI API key is not configured. Please set it in the code.\"\n",
        "        try:\n",
        "            client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "            response = client.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer the question based *only* on the provided context.\"}, {\"role\": \"user\", \"content\": f\"Context:\\n{retrieved_context}\\n\\nQuestion: {query}\"}],\n",
        "                temperature=0.1\n",
        "            )\n",
        "            return textwrap.fill(response.choices[0].message.content.strip(), width=100)\n",
        "        except Exception as e:\n",
        "            return f\"OpenAI Error: {e}\"\n",
        "\n",
        "# --- Function to re-index (crawl + process + save) data for a specific product ---\n",
        "# Arguments now correctly match the inputs from gr.State and gr.File\n",
        "def reindex_product_content(product_name: str, urls_to_crawl: list, uploaded_file_paths: list, progress=gr.Progress(track_tqdm=True)):\n",
        "    global global_context, chunk_texts, vector_index\n",
        "\n",
        "    # Define paths for this specific product's cache\n",
        "    product_faiss_index = os.path.join(BASE_CACHE_DIR, f\"{product_name}_faiss_index.bin\")\n",
        "    product_chunks_file = os.path.join(BASE_CACHE_DIR, f\"{product_name}_chunks.json\")\n",
        "\n",
        "    all_raw_texts = []\n",
        "    status = []\n",
        "\n",
        "    status.append(f\"--- Starting Re-Indexing for {product_name} ---\")\n",
        "\n",
        "    if urls_to_crawl:\n",
        "        status.append(f\"Crawling {len(urls_to_crawl)} URLs for {product_name}...\")\n",
        "        crawled_results = []\n",
        "        MAX_CONCURRENT_CRAWLS = 5 # Adjust as needed for your system\n",
        "        # Ensure progress.tqdm is used correctly\n",
        "        with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_CRAWLS) as executor:\n",
        "            # Use list() around executor.map to ensure tqdm can iterate over it\n",
        "            for result in progress.tqdm(list(executor.map(crawl_single_url, urls_to_crawl)), total=len(urls_to_crawl), desc=f\"Crawling {product_name} URLs\"):\n",
        "                crawled_results.append(result)\n",
        "\n",
        "        successful_crawls = 0\n",
        "        for url, text_content in crawled_results:\n",
        "            if not text_content.startswith(\"[Error\"):\n",
        "                all_raw_texts.append(f\"--- Document Source: {url} ---\\n{text_content}\\n\")\n",
        "                successful_crawls += 1\n",
        "        status.append(f\"Successfully extracted text from {successful_crawls} out of {len(crawled_results)} URLs for {product_name}.\")\n",
        "    else:\n",
        "        status.append(f\"No URLs provided for {product_name} re-crawling.\")\n",
        "\n",
        "    if uploaded_file_paths:\n",
        "        status.append(f\"Processing {len(uploaded_file_paths)} uploaded files...\")\n",
        "        for file_obj in uploaded_file_paths: # file_obj here is a Gradio File object\n",
        "            try:\n",
        "                extracted_text = extract_text_from_file(file_obj.name) # Pass the file path\n",
        "                all_raw_texts.append(f\"--- Document Source: {file_obj.name} ---\\n{extracted_text}\\n\")\n",
        "                status.append(f\"Successfully extracted text from: {file_obj.name}\")\n",
        "            except Exception as e:\n",
        "                status.append(f\"Error extracting text from {file_obj.name}: {e}\")\n",
        "\n",
        "    combined_text = \"\\n\\n\".join(all_raw_texts)\n",
        "    global_context[\"text\"] = combined_text # Update global raw text for the currently active KB\n",
        "\n",
        "    if not combined_text:\n",
        "        status.append(\"No content to process. Index will not be built/saved.\")\n",
        "        # Clear current active KB if no content was loaded\n",
        "        vector_index = None\n",
        "        chunk_texts = []\n",
        "        global_context[\"text\"] = \"\"\n",
        "        return \"\\n\".join(status)\n",
        "\n",
        "    processed_chunks = chunk_text(combined_text)\n",
        "\n",
        "    if not processed_chunks:\n",
        "        status.append(\"No valid chunks created from content. Index will not be built/saved.\")\n",
        "        # Clear current active KB if no valid chunks\n",
        "        vector_index = None\n",
        "        chunk_texts = []\n",
        "        global_context[\"text\"] = \"\"\n",
        "        return \"\\n\".join(status)\n",
        "\n",
        "    # Build index and save it for this product\n",
        "    index_status_message = build_faiss_index(processed_chunks)\n",
        "    status.append(index_status_message)\n",
        "\n",
        "    try:\n",
        "        faiss.write_index(vector_index, product_faiss_index)\n",
        "        with open(product_chunks_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(chunk_texts, f) # Save the globally updated chunk_texts\n",
        "        status.append(f\"‚úÖ FAISS index and chunks saved for {product_name} to '{product_faiss_index}' and '{product_chunks_file}'.\")\n",
        "    except Exception as e:\n",
        "        status.append(f\"‚ùå Error saving FAISS index or chunks for {product_name}: {e}\")\n",
        "\n",
        "    status.append(f\"--- Re-Indexing Process Complete for {product_name} ---\")\n",
        "    status.append(f\"Knowledge base for {product_name} is now active and ready for questions.\")\n",
        "    return \"\\n\".join(status)\n",
        "\n",
        "# --- Function to load a specific pre-built index at startup or after re-indexing ---\n",
        "def load_specific_index_from_disk(product_name):\n",
        "    global vector_index, chunk_texts, global_context\n",
        "    status_messages = []\n",
        "\n",
        "    product_faiss_index = os.path.join(BASE_CACHE_DIR, f\"{product_name}_faiss_index.bin\")\n",
        "    product_chunks_file = os.path.join(BASE_CACHE_DIR, f\"{product_name}_chunks.json\")\n",
        "\n",
        "    status_messages.append(f\"Attempting to load knowledge base for {product_name}...\")\n",
        "\n",
        "    if os.path.exists(product_faiss_index) and os.path.exists(product_chunks_file):\n",
        "        try:\n",
        "            vector_index = faiss.read_index(product_faiss_index)\n",
        "            with open(product_chunks_file, \"r\", encoding=\"utf-8\") as f:\n",
        "                chunk_texts = json.load(f)\n",
        "\n",
        "            global_context[\"text\"] = \" \".join(chunk_texts) # Approximation for chart if needed\n",
        "\n",
        "            status_messages.append(f\"‚úÖ Loaded FAISS index from '{product_faiss_index}' with {len(chunk_texts)} chunks.\")\n",
        "            status_messages.append(f\"System ready to answer questions about {product_name}.\")\n",
        "            status_messages.append(f\"Total characters in {product_name} index: {len(global_context['text']):,}\")\n",
        "        except Exception as e:\n",
        "            status_messages.append(f\"‚ùå Error loading pre-built index for {product_name}: {e}. Index will be empty.\")\n",
        "            vector_index = None\n",
        "            chunk_texts = []\n",
        "            global_context[\"text\"] = \"\"\n",
        "    else:\n",
        "        status_messages.append(f\"‚ö†Ô∏è Pre-built index files for {product_name} not found.\")\n",
        "        status_messages.append(f\"Please click 'Load/Refresh {product_name} Content' to build its knowledge base.\")\n",
        "        vector_index = None\n",
        "        chunk_texts = []\n",
        "        global_context[\"text\"] = \"\"\n",
        "\n",
        "    return \"\\n\".join(status_messages)\n",
        "\n",
        "\n",
        "# --- Initial load at application startup ---\n",
        "# We won't load any specific KB at startup by default,\n",
        "# the user will pick one via a button.\n",
        "# The initial status will prompt the user to load a KB.\n",
        "initial_app_status = \"Welcome! Please select a Zoho product to load its knowledge base by clicking a button.\"\n",
        "\n",
        "\n",
        "# STEP 10: Chat handler\n",
        "def chatbot_response(user_query, chat_history, model_choice):\n",
        "    chat_history = chat_history or []\n",
        "    bot_message_content = \"\"\n",
        "    if not vector_index: # Check for the FAISS index existence\n",
        "        bot_message_content = \"‚ö†Ô∏è The knowledge base is not loaded. Please click a 'Load/Refresh' button for a Zoho product to build/load it.\"\n",
        "    elif user_query.lower().strip() == \"chart:\":\n",
        "        if global_context[\"text\"]:\n",
        "            bot_message_content = generate_chart_from_csv_text(global_context[\"text\"])\n",
        "        else:\n",
        "            bot_message_content = \"‚ö†Ô∏è No combined text available for chart generation. Load content first.\"\n",
        "    else:\n",
        "        bot_message_content = ask_question(user_query, model_choice)\n",
        "\n",
        "    chat_history.append([user_query, bot_message_content])\n",
        "    return chat_history, \"\" # Return updated history and empty string to clear the input box\n",
        "\n",
        "# STEP 11: Gradio UI with specific buttons\n",
        "with gr.Blocks(theme=gr.themes.Soft(), css=\".gradio-container {max-width: 1200px !important;}\") as demo:\n",
        "    gr.Markdown(\"# üìö Zoho Product Intelli-Chat üöÄ\")\n",
        "    gr.Markdown(\"Select a Zoho product to load its knowledge base and ask questions.\")\n",
        "\n",
        "    with gr.Row(variant=\"panel\"):\n",
        "        with gr.Column(scale=1, min_width=380):\n",
        "            gr.Markdown(\"### ‚öôÔ∏è Controls\")\n",
        "\n",
        "            gr.Markdown(\"#### Load/Refresh Knowledge Bases\")\n",
        "            gr.Markdown(\"<sub>Click a button to crawl and index content for a specific Zoho product. This will save the knowledge base for faster loading next time.</sub>\")\n",
        "\n",
        "            with gr.Row():\n",
        "                load_books_button = gr.Button(\"üìö Load/Refresh Zoho Books\", variant=\"primary\")\n",
        "                load_inventory_button = gr.Button(\"üì¶ Load/Refresh Zoho Inventory\", variant=\"secondary\")\n",
        "            with gr.Row():\n",
        "                load_payroll_button = gr.Button(\"üí∞ Load/Refresh Zoho Payroll\", variant=\"secondary\")\n",
        "                load_creator_button = gr.Button(\"üõ†Ô∏è Load/Refresh Zoho Creator\", variant=\"secondary\")\n",
        "            with gr.Row():\n",
        "                load_analytics_button = gr.Button(\"üìä Load/Refresh Zoho Analytics\", variant=\"secondary\")\n",
        "\n",
        "            # NEW BUTTONS FOR ADDITIONAL PRODUCTS\n",
        "            with gr.Row():\n",
        "                load_people_button = gr.Button(\"üßë‚Äçü§ù‚Äçüßë Load/Refresh Zoho People\", variant=\"secondary\")\n",
        "                load_recruit_button = gr.Button(\"üìù Load/Refresh Zoho Recruit\", variant=\"secondary\")\n",
        "            with gr.Row():\n",
        "                load_crm_button = gr.Button(\"üíº Load/Refresh Zoho CRM\", variant=\"secondary\")\n",
        "\n",
        "\n",
        "            with gr.Accordion(\"üìÇ Upload Additional Files (Optional)\", open=False):\n",
        "                file_input = gr.File(\n",
        "                    file_types=[\".pdf\", \".txt\"],\n",
        "                    file_count=\"multiple\",\n",
        "                    label=\"Upload PDF/TXT Documents (for current selected KB)\"\n",
        "                )\n",
        "\n",
        "            # Initial status from startup\n",
        "            load_status = gr.Textbox(label=\"Status & Log\", interactive=False, lines=8, value=initial_app_status)\n",
        "\n",
        "            with gr.Accordion(\"üß† Choose Your AI Model\", open=True):\n",
        "                model_choice = gr.Dropdown(\n",
        "                    [\"Hugging Face (Free)\", \"OpenAI (GPT)\"],\n",
        "                    label=\"Select Generative Model\",\n",
        "                    value=\"Hugging Face (Free)\"\n",
        "                )\n",
        "\n",
        "            clear_button = gr.Button(\"üßπ Clear Chat & Status\", variant=\"secondary\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            gr.Markdown(\"### üí¨ Your Conversation\")\n",
        "            chatbot = gr.Chatbot(label=\"Chat History\", height=600)\n",
        "            with gr.Row():\n",
        "                question_input = gr.Textbox(show_label=False, placeholder=\"Ask a question...\", scale=8)\n",
        "                submit_button = gr.Button(\"Ask\", variant=\"primary\", scale=1)\n",
        "\n",
        "    # --- Event Handlers for product-specific buttons ---\n",
        "    # The 'progress' object will be automatically injected by Gradio because the fn signature expects it.\n",
        "    # No need to put gr.Progress() in the inputs list.\n",
        "\n",
        "    load_books_button.click(\n",
        "        fn=reindex_product_content,\n",
        "        inputs=[\n",
        "            gr.State(\"Zoho Books\"), # Passes the string \"Zoho Books\" to 'product_name'\n",
        "            gr.State(zoho_books_urls), # Passes the list zoho_books_urls to 'urls_to_crawl'\n",
        "            file_input # Passes the uploaded files to 'uploaded_file_paths'\n",
        "            # No gr.Progress() here!\n",
        "        ],\n",
        "        outputs=[load_status]\n",
        "    ).then(\n",
        "        fn=load_specific_index_from_disk,\n",
        "        inputs=[gr.State(\"Zoho Books\")],\n",
        "        outputs=[load_status]\n",
        "    )\n",
        "\n",
        "    load_inventory_button.click(\n",
        "        fn=reindex_product_content,\n",
        "        inputs=[\n",
        "            gr.State(\"Zoho Inventory\"),\n",
        "            gr.State(zoho_inventory_urls), # Now includes new Inventory URLs\n",
        "            file_input\n",
        "            # No gr.Progress() here!\n",
        "        ],\n",
        "        outputs=[load_status]\n",
        "    ).then(\n",
        "        fn=load_specific_index_from_disk,\n",
        "        inputs=[gr.State(\"Zoho Inventory\")],\n",
        "        outputs=[load_status]\n",
        "    )\n",
        "\n",
        "    load_payroll_button.click(\n",
        "        fn=reindex_product_content,\n",
        "        inputs=[\n",
        "            gr.State(\"Zoho Payroll\"),\n",
        "            gr.State(zoho_payroll_urls), # Now includes new Payroll URLs\n",
        "            file_input\n",
        "            # No gr.Progress() here!\n",
        "        ],\n",
        "        outputs=[load_status]\n",
        "    ).then(\n",
        "        fn=load_specific_index_from_disk,\n",
        "        inputs=[gr.State(\"Zoho Payroll\")],\n",
        "        outputs=[load_status]\n",
        "    )\n",
        "\n",
        "    load_creator_button.click(\n",
        "        fn=reindex_product_content,\n",
        "        inputs=[\n",
        "            gr.State(\"Zoho Creator\"),\n",
        "            gr.State(zoho_creator_urls),\n",
        "            file_input\n",
        "            # No gr.Progress() here!\n",
        "        ],\n",
        "        outputs=[load_status]\n",
        "    ).then(\n",
        "        fn=load_specific_index_from_disk,\n",
        "        inputs=[gr.State(\"Zoho Creator\")],\n",
        "        outputs=[load_status]\n",
        "    )\n",
        "\n",
        "    load_analytics_button.click(\n",
        "        fn=reindex_product_content,\n",
        "        inputs=[\n",
        "            gr.State(\"Zoho Analytics\"),\n",
        "            gr.State(zoho_analytics_urls),\n",
        "            file_input\n",
        "            # No gr.Progress() here!\n",
        "        ],\n",
        "        outputs=[load_status]\n",
        "    ).then(\n",
        "        fn=load_specific_index_from_disk,\n",
        "        inputs=[gr.State(\"Zoho Analytics\")],\n",
        "        outputs=[load_status]\n",
        "    )\n",
        "\n",
        "    # NEW EVENT HANDLERS FOR ADDITIONAL PRODUCTS\n",
        "    load_people_button.click(\n",
        "        fn=reindex_product_content,\n",
        "        inputs=[\n",
        "            gr.State(\"Zoho People\"),\n",
        "            gr.State(zoho_people_urls),\n",
        "            file_input\n",
        "        ],\n",
        "        outputs=[load_status]\n",
        "    ).then(\n",
        "        fn=load_specific_index_from_disk,\n",
        "        inputs=[gr.State(\"Zoho People\")],\n",
        "        outputs=[load_status]\n",
        "    )\n",
        "\n",
        "    load_recruit_button.click(\n",
        "        fn=reindex_product_content,\n",
        "        inputs=[\n",
        "            gr.State(\"Zoho Recruit\"),\n",
        "            gr.State(zoho_recruit_urls),\n",
        "            file_input\n",
        "        ],\n",
        "        outputs=[load_status]\n",
        "    ).then(\n",
        "        fn=load_specific_index_from_disk,\n",
        "        inputs=[gr.State(\"Zoho Recruit\")],\n",
        "        outputs=[load_status]\n",
        "    )\n",
        "\n",
        "    load_crm_button.click(\n",
        "        fn=reindex_product_content,\n",
        "        inputs=[\n",
        "            gr.State(\"Zoho CRM\"),\n",
        "            gr.State(zoho_crm_urls),\n",
        "            file_input\n",
        "        ],\n",
        "        outputs=[load_status]\n",
        "    ).then(\n",
        "        fn=load_specific_index_from_disk,\n",
        "        inputs=[gr.State(\"Zoho CRM\")],\n",
        "        outputs=[load_status]\n",
        "    )\n",
        "\n",
        "    # Corrected: Use .submit() for Textbox and .click() for Button\n",
        "    question_input.submit(\n",
        "        fn=chatbot_response,\n",
        "        inputs=[question_input, chatbot, model_choice],\n",
        "        outputs=[chatbot, question_input],\n",
        "    )\n",
        "\n",
        "    submit_button.click(\n",
        "        fn=chatbot_response,\n",
        "        inputs=[question_input, chatbot, model_choice],\n",
        "        outputs=[chatbot, question_input],\n",
        "    )\n",
        "\n",
        "    clear_button.click(lambda: ([], \"\", initial_app_status), None, [chatbot, question_input, load_status])\n",
        "\n",
        "demo.launch(debug=True, max_threads=10) # Added inbrowser=True for convenience during local development"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}